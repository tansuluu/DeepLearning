---
title: "DataMining"
output: html_document
---
##Content
Proposal
	We have collected data set about flat’s price in Bishkek. There are 5 variables, such  as: 
Rooms- numbers of rooms in the frat, in range(1-5), numeric
Area- flat area in m^2, numeric
Central_Heating- has the flat central heating or not(1,0),binary
Floor- on which floor, range(1-12),numeric
Seria - is it “Elitka” or not(1,0), binary

And the target is Price of flat in $.
There are about 2400 samples, which we get from [stroka.kg](https://stroka.kg/kupit-kvartiru/), We built a Linear regression model and Random forest model, then train them with our data set, to be able to predict flat’s Price in Bishkek. For users' comfort we have made small WEB application and mobile application, where users are able to write how many rooms they want, area,with central heating or without, choose the floor, and seria. After the application show ups the approximate price such flat in Bishkek.

##Literature review
We have searched for real-world applications which use data science to predict price of things, but it was not successfull, may be it's is hidden. But we have found some articles related with House price prediction, such as:
* btrgnbtrn
.. ⋅*[Predicting House Prices Using Linear Regressi](https://medium.com/africa-creates/predicting-house-prices-using-linear-regression-fe699d091e04), here he tried to predict house prices in Iowa.
..⋅⋅*[7 Use Cases For Data Science And Predictive Analytics](https://towardsdatascience.com/7-use-cases-for-data-science-and-predictive-analytics-e3616e9331f9), this article about, where now prediction is used and his strength.
⋅⋅*[Predicting Housing Prices using Advanced Regression Techniques](https://towardsdatascience.com/predicting-housing-prices-using-advanced-regression-techniques-8dba539f9abe), here he tells about house price problem in Kaggle, and how can we solve it, explains how to clear outliers, build right model for prediction.

##Methodology

###Importing libraries
```{r message=FALSE, warning=FALSE, paged.print=FALSE, results='hide'}
library(ggplot2)
library(corrplot)
library(dplyr)
library(missForest)
library(caret)
library(spdplyr)
library(psych)
library(ggplot2)
```

###Reading data
```{r}

train<-read.csv("my.csv")
test<-read.csv("my1.csv")
summary(train)

```
It's our dataset, here 6 variables and 2432 samples

###Determining NAs
```{r}
sapply(train, function(x){sum(is.na(x))})
print("test data")
sapply(test, function(x){sum(is.na(x))})

```

###Deleting NAs
```{r}
train<- train[complete.cases(train), ]
sapply(train, function(x){sum(is.na(x))})

```

###Let's look at correlation between variables:
```{r}
correl<- cor(train) 
corrplot(correl, type="upper")
```

###Histagrm plot

```{r}
#train <- train %>% mutate_if(is.integer, as.numeric)
train1<-train
train1$price<-NULL
train1$area<-NULL
multi.hist(train,nrow=NULL,ncol=NULL,density=TRUE,freq=FALSE,bcol="blue",
      dcol=c("green","red"),dlty=c("dashed","dotted"),
      main=NULL,mar=c(2,1,1,1),breaks=21)
```

###Boxplot for visualizing outliers

```{r}
boxplot(train1, col=c("#FF000099", "#FF6D0099"), 
  medcol=c("#FFDB00FF", "#B6FF00FF"), 
  whiskcol=c("#49FF00FF", "#00FF24FF"), 
  staplecol=c("#00FF92FF", "#00FFFFFF"), 
  boxcol=c("#0092FFFF", "#0024FFFF"), 
  outcol=c("#4900FFFF", "#B600FFFF"), 
  outbg=c("#FF00DB66", "#FF006D66"), 
  outcex=3, outpch=21)
boxplot(train$price,col=c("powderblue"),boxcol=c("#0092FFFF"),outcol=c("#4900FFFF"),outbg=c("#FF00DB66"))
boxplot(train$area,col=c("mistyrose"),boxcol=c("#B600FFFF"),outcol=c("#00FF24FF"),outbg=c("#49FF00FF"))

```

###Let's determine and remove outliers

```{r}
outliers <- boxplot(train$Elitka,col=c("green"))$out
train[which(train$Elitka %in% outliers),]
```

```{r}
train <- train[-which(train$Elitka %in% outliers),]
boxplot(train$Elitka,col=c("pink"))
```
so we have deleted outliers, We will not delete outliers in heating, because the outliers are all 0s;

###Now it's time to build model, firstly lets to Linera regression
```{r}
model<- lm(price ~ ., train)
#learn which variable to exclude
step(model,direction = "backward")

```
as we see all variables plays role in model.

###Prediction with test data
```{r}
predicted <- predict(model, test)
RMSE(test$price,predicted)
summary(model)
```

###Random forest regression model
```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, results='hide'}
modelRF <- randomForest(price ~ .,train,type = "regression",ntree = 277,
  do.trace = TRUE)
```

```{r}
#modelRF <- randomForest(price ~ .,train,type = "regression",ntree = 277, do.trace = TRUE)
predicted1 <- predict(modelRF, test)
RMSE(test$price,predicted1)
```

###Now we can predict flat price in Bishkek

##Result
####As result we have Linear regression and random forest reggresion models, which we have trained with out dataset, and test with test data. Linear regression's R^2=63 and Random forest's R^2 is 73.

##Reference

[stroka.kg](https://stroka.kg/kupit-kvartiru/), collected data from here.
[R Markdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf), cheat sheet to write beautiful markdown in R
[Linear reggresion](http://r-statistics.co/Linear-Regression.html), all information about linear regression
[Plot in r](https://rdrr.io/r/graphics/boxplot.matrix.html#heading-3), how to build plots in R




